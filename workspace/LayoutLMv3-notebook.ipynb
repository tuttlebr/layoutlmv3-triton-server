{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021126bb",
   "metadata": {},
   "source": [
    "# LayoutLMv3\n",
    "\n",
    "\n",
    "## Overview\n",
    "The LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei. LayoutLMv3 simplifies [LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2) by using patch embeddings (as in [ViT](https://huggingface.co/docs/transformers/model_doc/vit)) instead of leveraging a CNN backbone, and pre-trains the model on 3 objectives: masked language modeling (MLM), masked image modeling (MIM) and word-patch alignment (WPA).\n",
    "\n",
    "The abstract from the paper is the following:\n",
    "\n",
    "*Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis.*\n",
    "\n",
    "![layoutlmvs](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/layoutlmv3_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f09ad",
   "metadata": {},
   "source": [
    "## Imports and NVIDIA GPU Device Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import (\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    LayoutLMv3Model,\n",
    ")\n",
    "\n",
    "from pytesseract import apply_tesseract, iob_to_label, unnormalize_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch v{} running on {}\".format(torch.__version__, device))\n",
    "model_path = \"nielsr/layoutlmv3-finetuned-funsd\"\n",
    "preprocessor_path = \"microsoft/layoutlmv3-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8c92b",
   "metadata": {},
   "source": [
    "## Sample Image\n",
    "Also saved to `data.json` for use with `perf_analyzer` as default input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421661be",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sample.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "width, height = image.size\n",
    "\n",
    "buffered = BytesIO()\n",
    "image.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue())\n",
    "data_dict = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"raw_image_array\": {\n",
    "                \"content\": {\"b64\": \"{}\".format(img_str.decode(\"utf-8\"))},\n",
    "                \"shape\": [len(buffered.getvalue())],\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "with open(\n",
    "    \"/root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/layoutlmv3_inputs.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed691c6",
   "metadata": {},
   "source": [
    "## Tesseract Optical Character Recognition\n",
    "\n",
    "The implimentation of Tesseract-OCR within this container was [compiled](https://tesseract-ocr.github.io/tessdoc/TesseractOpenCL.html) to leverage OpenCL devices. It's decoupled from the Tokenizer to other OCR methods may be used if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\n",
    "    preprocessor_path, torchscript=True, apply_ocr=False\n",
    ")\n",
    "\n",
    "text, boxes = apply_tesseract(image, lang=\"eng\", tesseract_config=\"--oem 1\")\n",
    "\n",
    "encoding = processor(\n",
    "    image,\n",
    "    text=text,\n",
    "    boxes=boxes,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e37f3",
   "metadata": {},
   "source": [
    "## Instantiate the LayoutLMv3 Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43968e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in encoding.keys():\n",
    "    print(\"{} shape: {}\".format(i, encoding[i].shape))\n",
    "    print(\"{} dtype: {}\".format(i, encoding[i].dtype))\n",
    "    print(\"\")\n",
    "\n",
    "for k, v in encoding.items():\n",
    "    try:\n",
    "        encoding[k] = v.to(device)\n",
    "    except:\n",
    "        pass\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af01c59",
   "metadata": {},
   "source": [
    "## Instantiate a Trained LayoutLMv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    model_path, torchscript=True\n",
    ").to(device)\n",
    "id2label = model.config.id2label\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12e392",
   "metadata": {},
   "source": [
    "## Run inference to retrieve logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"data\": []}\n",
    "for i in encoding:\n",
    "    sub_data = {i: {}}\n",
    "    a = np.array(encoding.data[i].cpu())\n",
    "    sub_data[i][\"content\"] = a.flatten().tolist()\n",
    "    sub_data[\"shape\"] = list(a.shape)\n",
    "    data[\"data\"].append(sub_data)\n",
    "    print(\" - {}:{}\".format(i, list(a.shape)))\n",
    "\n",
    "with open(\n",
    "    \"/root/.cache/huggingface/triton-models/layoutlmv3_2_inference/layoutlmv3_inputs.json\",\n",
    "    \"w\",\n",
    ") as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f9296",
   "metadata": {},
   "source": [
    "## Parse Predictions to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs[0].argmax(-1).squeeze().tolist()\n",
    "token_boxes = encoding.bbox.squeeze().tolist()\n",
    "is_subword = np.array(offset_mapping.squeeze().tolist())[:, 0] != 0\n",
    "true_predictions = [\n",
    "    id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]\n",
    "]\n",
    "true_boxes = [\n",
    "    unnormalize_box(box, width, height)\n",
    "    for idx, box in enumerate(token_boxes)\n",
    "    if not is_subword[idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d105885",
   "metadata": {},
   "source": [
    "## Draw Bounding Boxes with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "label2color = {\n",
    "    \"question\": \"blue\",\n",
    "    \"answer\": \"green\",\n",
    "    \"header\": \"orange\",\n",
    "    \"other\": \"violet\",\n",
    "}\n",
    "\n",
    "for prediction, box in zip(true_predictions, true_boxes):\n",
    "    predicted_label = iob_to_label(prediction).lower()\n",
    "    draw.rectangle(box, outline=label2color[predicted_label])\n",
    "    draw.text(\n",
    "        (box[0] + 10, box[1] - 10),\n",
    "        text=predicted_label,\n",
    "        fill=label2color[predicted_label],\n",
    "        font=font,\n",
    "    )\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5be556",
   "metadata": {},
   "source": [
    "### Encoding Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cf509",
   "metadata": {},
   "source": [
    "### Offset Mapping Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2931b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0df45",
   "metadata": {},
   "source": [
    "### Model Output (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8093e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c54e88",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2deebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /root/.cache/huggingface/5a806f3a6ea0fadc67e6a7c9b86ee34d20cfb694fef7f5cf61e3e442aa87bf01.b3f43b13348b0046ddf48e57ee1f9bf6f5445d452d4401b53e88ec565b2e03d3 \\\n",
    "    /root/.cache/huggingface/layoutlmv3/pytorch_model.bin \\\n",
    "&& cp /root/.cache/huggingface/6a1296143bda78d4e76520a102633eab7d8c7a7436e0240740d583b13e513634.85a59041585b9df84cb2409000e75ec862472acc9fd2753e360422482468cdb3 \\\n",
    "    /root/.cache/huggingface/layoutlmv3/tokenizer_config.json \\\n",
    "&& cp /root/.cache/huggingface/12e3fbf8d2bc2a2331583c2b01603725959b563e70dac5da35e57975788ff9b9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05 \\\n",
    "    /root/.cache/huggingface/layoutlmv3/vocab.json \\\n",
    "&& cp /root/.cache/huggingface/93b2ea2c7da83bab15f33c9981644685853156b746135bc522c550c812d68b93.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b \\\n",
    "    /root/.cache/huggingface/layoutlmv3/merges.txt \\\n",
    "&& cp /root/.cache/huggingface/a8f2f8aefcea7536ff4a117fdedb45594a098cf9613afed040d5713f8c422150.ed72adea09fba297feb464926d4fd9dc8c8cd9fad692961cc38123e3316598e2 \\\n",
    "    /root/.cache/huggingface/layoutlmv3/config.json \\\n",
    "&& cp /root/.cache/huggingface/e90f549bb33101e8141284f850ad2b907e919417da746aca394c6a403dc4151f.4f4fbbd7db79618fdf8c9a37cf26bd2881f493d22820d058af4c37bb42d657ba \\\n",
    "    /root/.cache/huggingface/layoutlmv3/preprocessor_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m transformers.onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformers.onnx \\\n",
    "    --model=/root/.cache/huggingface/layoutlmv3/ \\\n",
    "    --atol=2e-4 \\\n",
    "    --opset=13 \\\n",
    "    --feature={\"token-classification\"} \\\n",
    "    --framework={\"pt\"} \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_2_inference/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef57565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /root/.cache/huggingface/6a1296143bda78d4e76520a102633eab7d8c7a7436e0240740d583b13e513634.85a59041585b9df84cb2409000e75ec862472acc9fd2753e360422482468cdb3 \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/1/preprocessing_config/tokenizer_config.json \\\n",
    "&& cp /root/.cache/huggingface/12e3fbf8d2bc2a2331583c2b01603725959b563e70dac5da35e57975788ff9b9.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05 \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/1/preprocessing_config/vocab.json \\\n",
    "&& cp /root/.cache/huggingface/93b2ea2c7da83bab15f33c9981644685853156b746135bc522c550c812d68b93.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/1/preprocessing_config/merges.txt \\\n",
    "&& cp /root/.cache/huggingface/a8f2f8aefcea7536ff4a117fdedb45594a098cf9613afed040d5713f8c422150.ed72adea09fba297feb464926d4fd9dc8c8cd9fad692961cc38123e3316598e2 \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/1/preprocessing_config/config.json \\\n",
    "&& cp /root/.cache/huggingface/e90f549bb33101e8141284f850ad2b907e919417da746aca394c6a403dc4151f.4f4fbbd7db79618fdf8c9a37cf26bd2881f493d22820d058af4c37bb42d657ba \\\n",
    "    /root/.cache/huggingface/triton-models/layoutlmv3_1_preprocess/1/preprocessing_config/preprocessor_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a3d34",
   "metadata": {},
   "source": [
    "___\n",
    "*You can close this container and run `docker compose up layoutlmv3-triton-server`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
